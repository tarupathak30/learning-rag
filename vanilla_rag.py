# -*- coding: utf-8 -*-
"""vanilla_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RkPt1bg3jf8zFX8oohDswMxZutzdEzU3
"""

!apt-get install -y tesseract-ocr
!pip install pillow pytesseract
!pip install "unstructured[all-docs]"
!pip install langchain_chroma langchain langchain-community
!pip install langchain-groq
!apt-get update && apt-get install -y poppler-utils

!pip install -U bitsandbytes transformers accelerate

import base64
import io
import hashlib
from typing import List

from PIL import Image
import pytesseract

from unstructured.partition.pdf import partition_pdf
from unstructured.documents.elements import Table, Image as UImage

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

llm = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

OCR_CACHE = {}
def ocr_images(image_base64_list):
    descriptions = []

    for i, img_b64 in enumerate(image_base64_list, start=1):

        if not img_b64:        # âœ… SAFETY CHECK
            continue

        key = hashlib.md5(img_b64.encode()).hexdigest()

        if key in OCR_CACHE:
            descriptions.append(OCR_CACHE[key])
            continue

        try:
            img_bytes = base64.b64decode(img_b64)
            img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
            text = pytesseract.image_to_string(img).strip()

            if text:
                desc = f"Figure {i} text: {text[:600]}"
            else:
                desc = f"Figure {i} is a diagram or illustration."

        except Exception:
            desc = f"Figure {i} could not be processed."

        OCR_CACHE[key] = desc
        descriptions.append(desc)

    return descriptions

def extract_modalities(chunk):
    text = chunk.text or ""
    tables = []
    images = []

    if hasattr(chunk.metadata, "orig_elements"):
        for el in chunk.metadata.orig_elements:
            if isinstance(el, Table):
                tables.append(
                    getattr(el.metadata, "text_as_html", el.text)
                )

            elif isinstance(el, UImage):
                img_b64 = getattr(el.metadata, "image_base64", None)
                if img_b64:              # âœ… THIS LINE MATTERS
                    images.append(img_b64)

    return text, tables, images

def summarize_chunk(text, tables=None, image_desc=None):
    prompt = f"""
You are a technical assistant.

Summarize the following content concisely.
Focus on definitions, mechanisms, and equations.

Text:
{text}
"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(llm.device)

    outputs = llm.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=False,
        temperature=0.0
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def ingest_pdf_to_chroma(
    pdf_path: str,
    persist_dir: str = "chroma_db"
):
    print("ðŸ“„ Parsing PDF...")
    raw_chunks = partition_pdf(
        filename=pdf_path,
        infer_table_structure=True,
        extract_images_in_pdf=True,
        chunking_strategy="by_title",
        max_characters=1500,
    )

    documents = []

    for i, chunk in enumerate(raw_chunks, start=1):
        print(f"ðŸ§  Processing chunk {i}/{len(raw_chunks)}")

        text, tables, images = extract_modalities(chunk)
        image_desc = ocr_images(images)

        if tables or image_desc:
            summary = summarize_chunk(text, tables, image_desc)
        else:
            summary = text

        documents.append(
            Document(
                page_content=summary,
                metadata={
                    "chunk_id": i,
                    "raw_text": text,
                    "source": pdf_path
                }
            )
        )

    print("ðŸ§¬ Creating embeddings + storing in Chroma...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    db = Chroma.from_documents(
      documents=documents,
      embedding=embeddings,
      persist_directory=persist_dir
  )
    print("âœ… Ingestion complete")
    return db

def search(db, query, k=3):
    retriever = db.as_retriever(search_kwargs={"k": k})
    return retriever.invoke(query)

from google.colab import files
files.upload()

# Run once
db = ingest_pdf_to_chroma("/content/1706.03762v7.pdf")

# Run many times
query = "How many attention heads does the Transformer use?"

docs = db.similarity_search(query, k=3)
for i, d in enumerate(docs):
    print(f"\n--- DOC {i+1} ---\n")
    print(d.page_content[:800])

def answer_question(chunks, query):
    # chunks is List[Document]
    context = "\n\n".join([doc.page_content for doc in chunks])

    prompt = f"""
<s>[INST]
You are a technical assistant.

Answer the question using ONLY the context below.
If the answer is not present, say "I don't know".

Context:
{context}

Question:
{query}
[/INST]
"""

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=4096
    ).to(llm.device)

    outputs = llm.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=False,
        temperature=0.0
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer.split("[/INST]")[-1].strip()

# Run many times
query = "What design choices distinguish the original Transformer from later variants?"
chunks = search(db, query)
print(answer_question(chunks, query))

def answer_no_rag(query):
    prompt = f"""
<s>[INST]
Answer the question as best as you can.
[/INST]

Question:
{query}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(llm.device)
    outputs = llm.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=False,
        temperature=0.0
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Run many times
query = "What design choices distinguish the original Transformer from later variants?"
chunks = search(db, query)
print(answer_no_rag(query))

def grounding_score(answer, chunks):
    context = " ".join(doc.page_content for doc in chunks).lower()
    answer_words = answer.lower().split()

    supported = sum(1 for w in answer_words if w in context)
    return supported / max(len(answer_words), 1)

answer_wo_rag = answer_no_rag(query)
no_rag = grounding_score(answer_wo_rag, chunks)

answer_w_rag = answer_question(chunks, query)
rag = grounding_score(answer_w_rag, chunks)

print("no rag answer ", no_rag)
print("rag answer ", rag)

from numpy import dot
from numpy.linalg import norm

def cosine_sim(a, b):
    return dot(a, b) / (norm(a) * norm(b) + 1e-8)

def relevance_score(answer, question, embedder):
    a_emb = embedder.embed_query(answer)
    q_emb = embedder.embed_query(question)
    return cosine_sim(a_emb, q_emb)

from langchain_community.embeddings import HuggingFaceEmbeddings

embedder = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"
)

no_rag_relevance_score = relevance_score(answer_wo_rag, query, embeddings)

rag_relevance_score = relevance_score(answer_w_rag, query, embedder)

print("without rag relevance score : ", no_rag_relevance_score)
print("with rag relevance score : ", rag_relevance_score)

