# -*- coding: utf-8 -*-
"""vanilla_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RkPt1bg3jf8zFX8oohDswMxZutzdEzU3
"""

import base64
import io
import hashlib
from typing import List

from PIL import Image
import pytesseract

from unstructured.partition.pdf import partition_pdf
from unstructured.documents.elements import Table, Image as UImage

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings

OCR_CACHE = {}
def ocr_images(image_base64_list):
    descriptions = []

    for i, img_b64 in enumerate(image_base64_list, start=1):

        if not img_b64:        # âœ… SAFETY CHECK
            continue

        key = hashlib.md5(img_b64.encode()).hexdigest()

        if key in OCR_CACHE:
            descriptions.append(OCR_CACHE[key])
            continue

        try:
            img_bytes = base64.b64decode(img_b64)
            img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
            text = pytesseract.image_to_string(img).strip()

            if text:
                desc = f"Figure {i} text: {text[:600]}"
            else:
                desc = f"Figure {i} is a diagram or illustration."

        except Exception:
            desc = f"Figure {i} could not be processed."

        OCR_CACHE[key] = desc
        descriptions.append(desc)

    return descriptions

def extract_modalities(chunk):
    text = chunk.text or ""
    tables = []
    images = []

    if hasattr(chunk.metadata, "orig_elements"):
        for el in chunk.metadata.orig_elements:
            if isinstance(el, Table):
                tables.append(
                    getattr(el.metadata, "text_as_html", el.text)
                )

            elif isinstance(el, UImage):
                img_b64 = getattr(el.metadata, "image_base64", None)
                if img_b64:              # âœ… THIS LINE MATTERS
                    images.append(img_b64)

    return text, tables, images

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0
)

def summarize_chunk(text, tables, image_desc):
    prompt = f"""
You are creating a SEARCH-OPTIMIZED summary.

TEXT:
{text}

TABLES:
{tables}

IMAGE DESCRIPTIONS:
{image_desc}

TASK:
- Extract key facts, numbers, definitions
- Include alternative keywords
- Mention questions this chunk can answer
- Max 300 words
"""

    return llm.invoke(prompt).content

def ingest_pdf_to_chroma(
    pdf_path: str,
    persist_dir: str = "chroma_db"
):
    print("ðŸ“„ Parsing PDF...")
    raw_chunks = partition_pdf(
        filename=pdf_path,
        infer_table_structure=True,
        extract_images_in_pdf=True,
        chunking_strategy="by_title",
        max_characters=1500,
    )

    documents = []

    for i, chunk in enumerate(raw_chunks, start=1):
        print(f"ðŸ§  Processing chunk {i}/{len(raw_chunks)}")

        text, tables, images = extract_modalities(chunk)
        image_desc = ocr_images(images)

        if tables or image_desc:
            summary = summarize_chunk(text, tables, image_desc)
        else:
            summary = text

        documents.append(
            Document(
                page_content=summary,
                metadata={
                    "chunk_id": i,
                    "raw_text": text,
                    "source": pdf_path
                }
            )
        )

    print("ðŸ§¬ Creating embeddings + storing in Chroma...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    db = Chroma.from_documents(
      documents=documents,
      embedding=embeddings,
      persist_directory=persist_dir
  )
    print("âœ… Ingestion complete")
    return db

def search(db, query, k=3):
    retriever = db.as_retriever(search_kwargs={"k": k})
    return retriever.invoke(query)

def answer_question(chunks, query):
    docs = db.similarity_search(query, k=5)

    context = ""
    for d in docs:
        context += d.metadata.get("raw_text", "") + "\n"

    prompt = f"""
Answer the question using ONLY the context below.
If the answer is not present, say you don't know.

IMPORTANT:
- Explicitly extract and state ALL numbers, dimensions, counts, and hyperparameters.
- If a number appears in text or tables, it MUST be written verbatim.


QUESTION:
{query}

CONTEXT:
{context}

ANSWER:
"""

    return llm.invoke(prompt).content

from google.colab import files
files.upload()

# Run once
db = ingest_pdf_to_chroma("/content/1706.03762v7.pdf")

# Run many times
query = "How many attention heads does the Transformer use?"

docs = db.similarity_search(query, k=3)
for i, d in enumerate(docs):
    print(f"\n--- DOC {i+1} ---\n")
    print(d.page_content[:800])

# Run many times
query = "What trade-offs are discussed regarding attention head count?"
chunks = search(db, query)
print(answer_question(chunks, query))

